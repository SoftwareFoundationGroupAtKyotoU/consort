\section{Experiments}
\label{sec:eval}

We now present the results of some preliminary experiments performed with the implementation
described in \Cref{sec:infr}. The goal of these experiments was to answer the following questions:
\begin{enumerate}
\item Is the type system (and extensions of \Cref{sec:infr}) expressive enough to type and verify non-trivial programs?
\item Is type inference feasible?
\end{enumerate}

To answer these questions, we evaluated our prototype implementation
on two sets of benchmarks. The first are a set of data structure
implementations and microbenchmarks written directly in our low-level
imperative language. We developed this suite to
test the expressive power of our type system and inference.
The programs included in this suite are:
\begin{itemize}
\item \textbf{Array-List} Implementation of an unbounded list backed by an array.
\item \textbf{Sorted-List} Implementation of an in-place insertion sort algorithm.
\item \textbf{Shuffle} Multiple live references are used to mutate the same location in program memory as in \Cref{exmp:shuffle-example}.
\item \textbf{Mutable-List} Implementation of general linked lists with a clear operation.
\item \textbf{Array-Inv} A program which allocates a length $n$ array and writes the value $i$ at index $i$.
\end{itemize}
We also introduced unsafe mutations to these programs to
check our tool for unsoundness.

\begin{table}[t]
  \caption{Description of benchmark suite adapted from JayHorn. \textbf{Java} are omitted programs
    that test Java specific features. \textbf{Inc} are tests that cannot be handled by \name, e.g.,
    null checking, pointer equality, etc. \textbf{Bug} includes a ``safe'' program
    we discovered was actually incorrect.}
  \begin{center}
    \input{benchmark_table}
  \end{center}
  \label{tab:breakdown}
\end{table}

For our second benchmark suite, we adapted the benchmark suite
of JayHorn \cite{kahsai2017quantified,kahsai2016jayhorn}, a verification tool for Java.
This test suite contains a combination of 82 safe and unsafe programs written in
Java. We chose this benchmark suite as, like \name, JayHorn
is concerned with the automated, flow-sensitive verification of
programs in a language with mutable, managed memory.
Further, although some of their benchmark programs tested Java
specific features, most could be adapted
into our low-level language.
A detailed breakdown of the adapted benchmark suite can be found in \Cref{tab:breakdown}

\begin{remark}
  The original JayHorn paper includes two additional benchmark sets, Mine Pump and CBMC.
  Both our tool and the most recent version (v0.6.1) of JayHorn time out on the Mine Pump benchmark. Further,
  the CBMC tests were either subsumed by our own test programs, tested Java specific
  features, or tested program synthesis functionality. We therefore omitted both of these
  benchmarks from our evaluation as we considered the results uninteresting.
\end{remark}

\subsubsection{Experimental Setup}
We first ran \name on each program in our benchmark suite. For each program in our suite,
we recorded the output of our tool as well as the end-to-end runtime. For the JayHorn suite,
we first ran our tool on the adapted version of each test program and then ran the most recent\footnote{As of October 15, 2019}
development build of JayHorn\footnote{We did not use the most recent official release of JayHorn as
  it does not include critical fixes for soundness bugs \url{https://github.com/jayhorn/jayhorn/issues/154}.} on the corresponding
Java version.
As run time comparisons are meaningless given the differences in problem domains,
we collected only the final verification result for this set of experiments. All tests
were run on a machine with 16 GB RAM and 4 CPUs at 2GHz and with a timeout of 60 seconds.

\subsection{Results}
The results of our experiments are shown in \Cref{tab:jh-results}. On the JayHorn benchmark
suite \name performs competitively with JayHorn, correctly identifying 29 of the 32 safe programs
as such. For all 3 tests on which \name timed out after 60 seconds, JayHorn also timed out or encountered an exception
(columns \emph{T/O} and \emph{Err.}).
For the unsafe programs, \name correctly identified all programs as unsafe within 60 seconds;
JayHorn timed out on one test, and answered \textsc{Unknown} for 6 others (column \emph{Imprecise}).

On our own benchmark set, \name correctly verifies all safe versions
of the programs within 60 seconds. For the unsafe variants,
\name was able to quickly and definitively determine these programs unsafe.

\begin{table}[t]
  \caption{Comparison of \name to JayHorn on the benchmark set of \cite{kahsai2017quantified} and the results on our custom benchmark suite.}
  \begin{center}
    \input{jayhorn_table}
    \input{consort_table}
  \end{center}
  \label{tab:jh-results}
\end{table}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:

%  LocalWords:  JayHorn

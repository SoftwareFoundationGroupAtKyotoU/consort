\section{Experiments}
\label{sec:eval}

We now present the results of some preliminary experiments performed with the implementation
described in \Cref{sec:infr}. The goal of these experiments was to answer the following questions:
\begin{enumerate}
\item Is the type system (and extensions of \Cref{sec:infr}) expressive enough to type and verify non-trivial programs?
\item Is type inference feasible?
\end{enumerate}

\begin{table}[t]
  \caption{Description of benchmark suite adapted from JayHorn. \textbf{Java} are omitted programs
    that test Java specific features. \textbf{Inc} are tests that cannot be handled by \name, e.g.,
    null checking, pointer equality, etc. \textbf{Bug} includes a ``safe'' program
    we discovered was actually incorrect.}
  \begin{center}
    \input{benchmark_table}
  \end{center}
  \label{tab:breakdown}
\end{table}

To answer these questions, we evaluated our prototype implementation
on two sets of benchmarks. 
The first set is adapted from JayHorn \cite{kahsai2017quantified,kahsai2016jayhorn},
a verification tool for Java; like other many other tools,
JayHorn adapts the abstract location based approach
described in \Cref{sec:intro}.
This test suite contains a combination of 82 safe and unsafe programs written in
Java. We chose this benchmark suite as, like \name, JayHorn
is concerned with the automated, flow-sensitive verification of
programs in a language with mutable, managed memory.
Further, although some of their benchmark programs tested Java
specific features, most could be adapted
into our low-level language.
Those tests we could adapt allow a comparison with existing state-of-the-art
verification techniques.
A detailed breakdown of the adapted benchmark suite can be found in \Cref{tab:breakdown}

\begin{remark}
  The original JayHorn paper includes two additional benchmark sets, Mine Pump and CBMC.
  Both our tool and the most recent version (v0.6.1) of JayHorn time out on the Mine Pump benchmark. Further,
  the CBMC tests were either subsumed by our own test programs, tested Java specific
  features, or tested program synthesis functionality. We therefore omitted both of these
  benchmarks from our evaluation.
\end{remark}

The second benchmark set are data structure
implementations and microbenchmarks written directly in our low-level
imperative language. We developed this suite to
test the expressive power of our type system and inference.
The programs included in this suite are:
\begin{itemize}
\item \textbf{Array-List} Implementation of an unbounded list backed by an array.
\item \textbf{Sorted-List} Implementation of an in-place insertion sort algorithm.
\item \textbf{Shuffle} Multiple live references are used to mutate the same location in program memory as in \Cref{exmp:shuffle-example}.
\item \textbf{Mutable-List} Implementation of general linked lists with a clear operation.
\item \textbf{Array-Inv} A program which allocates a length $n$ array and writes the value $i$ at index $i$.
\item \textbf{Intro2} The motivating program shown in \Cref{fig:hard-loop} in \Cref{sec:intro}.
\end{itemize}
We also introduced unsafe mutations to these programs to
check our tool for unsoundness. We also translated these benchmarks
to Java for further comparison with JayHorn.

\subsubsection{Experimental Setup}
We first ran \name on each program in our benchmark suite and ran the
most recent\footnote{As of October 15, 2019} development build of JayHorn\footnote{We
  did not use the most recent official release of JayHorn because
  it does not include critical fixes for soundness bugs \url{https://github.com/jayhorn/jayhorn/issues/154}.} 
on the corresponding Java version. We recorded the final verification
result for both our tool and JayHorn. We also collected the
end-to-end runtime of \name for each test;
we do not give a performance comparison with
JayHorn given the many differences in target languages.
For the JayHorn suite, we first ran our tool on the adapted version of
each test program and ran JayHorn on the original Java version. We did not
collect runtime information for this set of experiments because our goal
is a comparison of tool precision, not performance.
All tests were run on a machine with 16 GB RAM and 4 CPUs at 2GHz
and with a timeout of 60 seconds (the same timeout
was used in \cite{kahsai2017quantified}). We used \name's  parallel backend (\cref{sec:infr})
with Z3 version 4.8.4 and HoICE version ...  \KS{Do not forget to fill this} and JayHorn's Eldarica backend.\footnote{We also tried the Spacer backend
  included with JayHorn, but it required an extremely old version of Z3 (4.3.2) and crashed the JVM on several of our tests.}

\subsection{Results}
The results of our experiments are shown in \Cref{tab:jh-results}. On the JayHorn benchmark
suite \name performs competitively with JayHorn, correctly identifying 29 of the 32 safe programs
as such. For all 3 tests on which \name timed out after 60 seconds, JayHorn also timed out or encountered an exception
(columns \emph{T/O} and \emph{Err.}).
For the unsafe programs, \name correctly identified all programs as unsafe within 60 seconds;
JayHorn timed out on one test, and answered \textsc{Unknown} for 6 others (column \emph{Imprecise}).

On our own benchmark set, \name correctly verifies all safe versions
of the programs within 60 seconds. For the unsafe variants,
\name was able to quickly and definitively determine these programs unsafe.
JayHorn times out on all tests except for \textbf{Shuffle}. We investigated
the cause of time outs and discovered that after verification failed
with an unbounded heap model, JayHorn attempts verification on increasingly
larger bounded heaps. In every case, JayHorn exceeded the 60 second timeout
before reaching heap limit bound. This suggests JayHorn struggles in the presence
of per-object invariants and unbounded allocations; in fact,
the only test JayHorn successfully verified (Shuffle) contains
just a single object allocation. We do not believe
this struggle is indicative of a shortcoming in JayHorn's implementation,
but stems from the fundamental limitations of the abstract
location based approach.

\begin{table}[t]
  \caption{Comparison of \name to JayHorn on the benchmark set of \cite{kahsai2017quantified} and our custom benchmark suite.}
  \begin{center}
    \input{jayhorn_table}
    \input{consort_table}
  \end{center}
  \label{tab:jh-results}
\end{table}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:

%  LocalWords:  JayHorn
